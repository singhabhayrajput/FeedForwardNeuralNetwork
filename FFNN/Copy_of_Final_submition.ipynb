{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "rd2MNkoS-ma8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff094647-e12f-44ea-a5e3-30f73939795c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8T9Nwie4F1At",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06b73bb4-e286-49e1-d5af-8395195efcec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5800.4757001511925\n",
            "5768.422680800926\n",
            "5706.291654656262\n",
            "5579.988212821281\n",
            "5325.512483356323\n",
            "4839.25676066601\n",
            "4010.469741171\n",
            "2853.8282114534704\n",
            "1656.9651462763165\n",
            "800.1253284645722\n",
            "373.00588462990027\n",
            "210.6545440763404\n",
            "156.63577432753104\n",
            "138.71029260606318\n",
            "132.2775645664699\n",
            "129.73039777234072\n",
            "128.6343551384706\n",
            "128.1292246317424\n",
            "127.87799871712221\n",
            "127.73853933109511\n",
            "127.64838175861907\n",
            "127.5795871349064\n",
            "127.51963441722346\n",
            "127.4629756972302\n",
            "127.4072254507896\n",
            "127.35142639010509\n",
            "127.29525891800589\n",
            "127.2386813641637\n",
            "127.18176714570605\n",
            "127.12463169240728\n",
            "127.06740021532904\n",
            "127.01019395103161\n",
            "126.95312469993354\n",
            "126.89629308430293\n",
            "126.83978841402464\n",
            "126.78368922663668\n",
            "126.72806407558531\n",
            "126.67297240044692\n",
            "126.61846537742034\n",
            "126.56458672659848\n",
            "126.51137346185675\n",
            "126.4588565803024\n",
            "126.40706169242269\n",
            "126.35600959587421\n",
            "126.30571679656258\n",
            "126.25619598088726\n",
            "126.2074564429925\n",
            "126.15950447074177\n",
            "126.11234369393765\n",
            "126.06597539808868\n",
            "126.02039880681345\n",
            "125.97561133573484\n",
            "125.93160882050644\n",
            "125.88838572139318\n",
            "125.84593530663481\n",
            "125.8042498166242\n",
            "125.7633206107562\n",
            "125.72313829863992\n",
            "125.68369285720914\n",
            "125.64497373512864\n",
            "125.60696994575954\n",
            "125.56967014982635\n",
            "125.53306272883104\n",
            "125.49713585013998\n",
            "125.46187752459875\n",
            "125.42727565743131\n",
            "125.39331809312098\n",
            "125.35999265488468\n",
            "125.32728717930722\n",
            "125.29518954663776\n",
            "125.26368770720153\n",
            "125.2327697043351\n",
            "125.20242369421791\n",
            "125.17263796292036\n",
            "125.1434009409799\n",
            "125.11470121575738\n",
            "125.08652754182995\n",
            "125.05886884962145\n",
            "125.03171425247551\n",
            "125.00505305233898\n",
            "124.97887474421691\n",
            "124.95316901953707\n",
            "124.92792576854836\n",
            "124.90313508187147\n",
            "124.87878725129666\n",
            "124.85487276992819\n",
            "124.83138233174515\n",
            "124.8083068306646\n",
            "124.7856373591629\n",
            "124.7633652065151\n",
            "124.74148185671339\n",
            "124.71997898609636\n",
            "124.69884846074424\n",
            "124.67808233367015\n",
            "124.65767284184278\n",
            "124.63761240306903\n",
            "124.61789361276499\n",
            "124.59850924063605\n",
            "124.57945222728767\n",
            "124.56071568078706\n",
            "124.5422928731888\n",
            "124.52417723704022\n",
            "124.50636236188058\n",
            "124.4888419907424\n",
            "124.4716100166658\n",
            "124.45466047923642\n",
            "124.43798756114877\n",
            "124.42158558480692\n",
            "124.40544900896728\n",
            "124.3895724254214\n",
            "124.37395055573403\n",
            "124.35857824803078\n",
            "124.34345047384015\n",
            "124.3285623249936\n",
            "124.31390901058758\n",
            "124.29948585400152\n",
            "124.28528828998273\n",
            "124.2713118617874\n",
            "124.25755221839191\n",
            "124.24400511176091\n",
            "124.23066639418245\n",
            "124.21753201566335\n",
            "124.2045980213893\n",
            "124.1918605492467\n",
            "124.17931582740357\n",
            "124.16696017195467\n",
            "124.15478998462439\n",
            "124.14280175053044\n",
            "124.13099203600189\n",
            "124.11935748645811\n",
            "124.10789482434377\n",
            "124.09660084711636\n",
            "124.08547242528705\n",
            "124.07450650051996\n",
            "124.06370008377431\n",
            "124.05305025350422\n",
            "124.04255415390443\n",
            "124.0322089932045\n",
            "124.02201204201076\n",
            "124.01196063169097\n",
            "124.00205215280843\n",
            "123.99228405359548\n",
            "123.98265383847051\n",
            "123.97315906659826\n",
            "123.96379735048521\n",
            "123.95456635461873\n",
            "123.94546379414314\n",
            "123.93648743357124\n",
            "123.92763508553215\n",
            "123.91890460955439\n",
            "123.91029391088422\n",
            "123.90180093933371\n",
            "123.89342368816436\n",
            "123.88516019299905\n",
            "123.877008530766\n",
            "123.86896681867086\n",
            "123.86103321319973\n",
            "123.85320590914392\n",
            "123.84548313866117\n",
            "123.8378631703528\n",
            "123.83034430837394\n",
            "123.8229248915635\n",
            "123.81560329260154\n",
            "123.80837791718682\n",
            "123.80124720323829\n",
            "123.79420962011974\n",
            "123.78726366788229\n",
            "123.78040787653156\n",
            "123.77364080531072\n",
            "123.76696104200666\n",
            "123.76036720227202\n",
            "123.75385792896779\n",
            "123.74743189152201\n",
            "123.74108778530612\n",
            "123.73482433102981\n",
            "123.72864027414772\n",
            "123.72253438428837\n",
            "123.71650545469055\n",
            "123.71055230166301\n",
            "123.70467376405016\n",
            "123.69886870271995\n",
            "123.6931360000588\n",
            "123.68747455948184\n",
            "123.68188330495872\n",
            "123.67636118054736\n",
            "123.67090714994346\n",
            "123.66552019603903\n",
            "123.6601993204939\n",
            "123.6549435433184\n",
            "123.64975190246615\n",
            "123.64462345343726\n",
            "123.63955726889239\n",
            "123.63455243827535\n",
            "123.62960806744675\n",
            "123.624723278326\n",
            "123.61989720854284\n",
            "123.61512901109828\n",
            "123.61041785403191\n",
            "123.60576292009964\n",
            "123.6011634064582\n",
            "RMSE on dev data: 1601.90006\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "np.random.seed(42)\n",
        "y_min=0\n",
        "NUM_FEATS = 72\n",
        "class Net(object):\n",
        "  def __init__(self, num_layers, num_units):\n",
        "    self.num_layers = num_layers\n",
        "    self.num_units = num_units\n",
        "    self.biases = []\n",
        "    self.weights = []\n",
        "    for i in range(num_layers):\n",
        "      if i==0:\n",
        "        self.weights.append(np.random.uniform(-1, 1, size=(NUM_FEATS, self.num_units)))\n",
        "      else:\n",
        "        self.weights.append(np.random.uniform(-1, 1, size=(self.num_units, self.num_units)))\n",
        "        self.biases.append(np.random.uniform(-1, 1, size=(self.num_units, 1)))\n",
        "    self.biases.append(np.random.uniform(-1, 1, size=(1, 1)))\n",
        "    self.weights.append(np.random.uniform(-1, 1, size=(self.num_units, 1)))\n",
        "\n",
        "  def __call__(self, X):\n",
        "    Y_pred = []\n",
        "    w=self.weights\n",
        "    b=self.biases\n",
        "    k=0\n",
        "\n",
        "    H=[]\n",
        "    H.append(X)\n",
        "\n",
        "    for i in range(self.num_layers+1):\n",
        "      A=[]\n",
        "      if(i==0):\n",
        "        c=np.maximum(0,np.dot(list(H[0]),w[0]))\n",
        "        H.append(c)\n",
        "      elif(i==self.num_layers):\n",
        "        c=np.dot(list(H[i]),w[i])+b[i-1].T\n",
        "        H.append(c)\n",
        "      else:\n",
        "        c=np.maximum(0,np.dot(list(H[i]),w[i]))+b[i-1].T\n",
        "        H.append(c)\n",
        "    Y_pred.append(H[-1])\n",
        "    return Y_pred[0],H\n",
        "  def backward(self,y_original,y_pred, HH1, lamda):\n",
        "    node_d_w,weight_d_w,bias_d_w=[],[],[]\n",
        "    w=np.einsum('ij,ik->ijk',HH1[-2],(2*(y_pred-y_original)))+2*0.1*self.weights[-1]\n",
        "    n=np.dot(self.weights[-1],(2*(y_pred-y_original).T)).T\n",
        "    node_d_w.append(n)\n",
        "    weight_d_w.append(w)\n",
        "    bias_d_w.append(2*(y_pred-y_original))\n",
        "    for i in range(self.num_layers-1):\n",
        "      bias_d_w.append(n)\n",
        "      w=np.einsum('ij,ik->ijk',node_d_w[i],HH1[self.num_layers-i-1])+2*0.01*self.weights[self.num_layers-i-1]\n",
        "      n=node_d_w[i]@self.weights[self.num_layers-i-1]\n",
        "      node_d_w.append(n)\n",
        "      weight_d_w.append(w)\n",
        "    w=np.einsum('ij,ik->ijk',HH1[0],node_d_w[-1])+2*0.01*self.weights[0]\n",
        "    weight_d_w.append(w)\n",
        "    return weight_d_w,node_d_w,bias_d_w\n",
        "\n",
        "class Optimizer(object):\n",
        "  def __init__(self, learning_rate):\n",
        "    self.l_r=learning_rate\n",
        "  def step(self, weights, biases, delta_weights, delta_biases):\n",
        "    for i in range(len(delta_weights)):\n",
        "        weights[-i-1]=weights[-i-1]-self.l_r*(np.sum(delta_weights[i],axis=0)/32)\n",
        "    # for i in range(len(b)):\n",
        "    #     bb=np.sum(delta_biases[i],axis=0)/32\n",
        "    #     bb=np.reshape(bb,(1, bb.size)).T\n",
        "    #     biases[-i-1]=biases[-i-1]-self.l_r*0.00001*bb\n",
        "    return weights,biases\n",
        "\n",
        "\n",
        "\n",
        "def loss_mse(y, y_hat):\n",
        "  return (np.sum(np.square(y_hat-y))/len(y))\n",
        "\n",
        "def loss_regularization(weights, biases):\n",
        "  return np.square(weights[-1])\n",
        "\n",
        "def loss_fn(y, y_hat, weights, biases, lamda):\n",
        "  return (loss_mse(y,y_hat)+lamda*loss_regularization(weights,biases))\n",
        "\n",
        "def rmse(y,y_hat):\n",
        "  return (math.sqrt(np.sum(np.square(y_hat-y))/len(y)))\n",
        "\n",
        "def cross_entropy_loss(y, y_hat):\n",
        "  return (np.mean(-np.log(y_hat[range(len(y_hat)), np.argmax(y,axis=1)])))\n",
        "\n",
        "\n",
        "def train(net, optimizer, lamda, batch_size, max_epochs,train_input, train_target,dev_input, dev_target):\n",
        "  for pp in range(max_epochs):\n",
        "    d=1\n",
        "    epoch_loss = 0.\n",
        "    k=0\n",
        "    while(d<len(train_input)-batch_size+1):\n",
        "      batch_input = train_input[d:d+batch_size]\n",
        "      batch_target = train_target[d:d+batch_size]\n",
        "      batch_target = np.reshape(batch_target,(1, batch_target.size)).T\n",
        "      Y_pred,HH=net.__call__(batch_input)\n",
        "      w,n,b=net.backward(batch_target,Y_pred,HH,lamda)\n",
        "      w,b=optimizer.step(net.weights,net.biases,w,b)\n",
        "      net.weights=w\n",
        "      net.biases=b\n",
        "      # batch_loss=loss_fn(batch_target, Y_pred, net.weights, net.biases, lamda)\n",
        "      batch_loss=loss_mse(batch_target, Y_pred)\n",
        "      epoch_loss+=batch_loss\n",
        "      d=d+batch_size\n",
        "      k+=1\n",
        "    print(epoch_loss/(k))\n",
        "  dev_pred,HH=net.__call__(dev_input)\n",
        "  dev_rmse = rmse(dev_target, dev_pred)\n",
        "  print('RMSE on dev data: {:.5f}'.format(dev_rmse))\n",
        "\n",
        "test_pred=[]\n",
        "def get_test_data_predictions(net, inputs):\n",
        "  Y_test_pred=net.__call__(inputs)\n",
        "  piyo=[]\n",
        "  for i in range(len(Y_test_pred)):\n",
        "    piyo.append(Y_test_pred[i][0]+y_min)\n",
        "\n",
        "  test_pred.append(0)\n",
        "  for i in range(len(piyo)):\n",
        "    test_pred.append((piyo[i][0]))\n",
        "def normalization(X):\n",
        "  max_element = np.max(X)\n",
        "  min_element = np.min(X)\n",
        "  #print(max_element,min_element)\n",
        "  X = (X-min_element)/((max_element-min_element)*100)\n",
        "  return X\n",
        "def read_data():\n",
        "  train_X = np.genfromtxt(\"drive/My Drive/FML_Assignment/DataSet/train.csv\", delimiter=',')\n",
        "  train_target=train_X[ : ,0]\n",
        "  abhi=feature_extraction(train_X)\n",
        "  x=train_X.T\n",
        "  list1=[]\n",
        "  list2=[]\n",
        "  for i in range(len(x)):\n",
        "    if i in abhi:\n",
        "      list1.append(x[i])\n",
        "    else:\n",
        "      list2.append(x[i])\n",
        "  train_X=np.array(list1).T\n",
        "\n",
        "  dev_X =  np.genfromtxt(\"drive/My Drive/FML_Assignment/DataSet/dev.csv\",delimiter=',')\n",
        "  dev_target = dev_X[ : ,0]\n",
        "  x=dev_X.T\n",
        "  list1=[]\n",
        "  list2=[]\n",
        "  for i in range(len(x)):\n",
        "    if i in abhi:\n",
        "      list1.append(x[i])\n",
        "    else:\n",
        "      list2.append(x[i])\n",
        "  dev_X=np.array(list1).T\n",
        "  dev_input = dev_X\n",
        "  dev_input=dev_input[1:]\n",
        "  dev_target=dev_target[1:]\n",
        "  test_input =  np.genfromtxt(\"drive/My Drive/FML_Assignment/DataSet/test.csv\",delimiter=',')\n",
        "  x=test_input.T\n",
        "  list1=[]\n",
        "  list2=[]\n",
        "  for i in range(len(x)+1):\n",
        "    if i in abhi:\n",
        "      list1.append(x[i-1])\n",
        "    else:\n",
        "      list2.append(x[i-1])\n",
        "  test_input=np.array(list1).T\n",
        "  train_X=normalization(train_X[1:])\n",
        "  dev_input=normalization(dev_input)\n",
        "  test_input=normalization(test_input[1:])\n",
        "  y_max=max(train_target[1:])\n",
        "  y_min=min(train_target[1:])\n",
        "  train_target=train_target-y_min\n",
        "  dev_target=dev_target-y_min\n",
        "  return train_X, train_target, dev_input, dev_target, test_input\n",
        "\n",
        "def feature_extraction(train_X):\n",
        "  q=train_X[:,1:]\n",
        "  p=train_X[:,1:]\n",
        "  mn=train_X[0]\n",
        "  mn=mn[0:90]\n",
        "  dl=pd.DataFrame(data=q,columns=mn)\n",
        "  count=0;\n",
        "  corr_matrix=dl.corr()\n",
        "  col_corr=set()\n",
        "  for i in range(len(corr_matrix.columns)):\n",
        "    for j in range(i):\n",
        "      if abs(corr_matrix.iloc[i,j]) > 0.5:\n",
        "        #print(i+1,j+1)\n",
        "        count=count+1\n",
        "        colname=corr_matrix.columns[i]\n",
        "        col_corr.add(colname)\n",
        "  abh=[]\n",
        "  for i in mn:\n",
        "    abh.append(float(i))\n",
        "  stri_ans=[]\n",
        "  feature_num=[]\n",
        "  for i in abh:\n",
        "    if i in col_corr:\n",
        "      c=1\n",
        "    else:\n",
        "      feature_num.append(int(i))\n",
        "      if(int(i)<13):\n",
        "        stri_ans.append(\"TimbreAvg\"+str(int(i)))\n",
        "      else:\n",
        "        stri_ans.append(\"TimbreCovariance\"+str(int(i)))\n",
        "  return feature_num\n",
        "def main():\n",
        "  max_epochs=200\n",
        "  batch_size = 32\n",
        "  learning_rate = 0.0001\n",
        "  num_layers = 1\n",
        "  num_units = 64\n",
        "  lamda = 0.1 # Regularization Parameter\n",
        "  train_input, train_target, dev_input, dev_target, test_input = read_data()\n",
        "  net = Net(num_layers, num_units)\n",
        "  optimizer = Optimizer(learning_rate)\n",
        "  train(\n",
        "      net, optimizer, lamda, batch_size, max_epochs,\n",
        "      train_input, train_target,\n",
        "      dev_input, dev_target\n",
        "      )\n",
        "  get_test_data_predictions(net, test_input)\n",
        "\n",
        "main()\n",
        "\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_input, train_target, dev_input, dev_target, test_input = read_data()"
      ],
      "metadata": {
        "id": "cmIwXT-WwEyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_input.shape"
      ],
      "metadata": {
        "id": "hyhmeDBjvo0E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f905dbf-92ef-4613-d168-8f663da30047"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40800, 72)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M4nGOYETGwm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pd.DataFrame(test_pred).to_csv(\"drive/My Drive/FML_Assignment/Cmest_pred.csv\")"
      ],
      "metadata": {
        "id": "0KHsVqnI8WTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XstiH1Rp8W-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fxw9bJ808YUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gliz5gQeF58x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}